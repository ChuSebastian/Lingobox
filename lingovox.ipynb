{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Speech To Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\Lingo\\myenv\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\PC\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.0.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: es (1.00) in first 30s of audio...\n",
      " Siempre me ha preocupado enormemente ser entendido y aceptado. -> start: 0.59, end: 5.736\n",
      "Pero he aprendido que agradar o contentar a todo el mundo es imposible. -> start: 5.736, end: 11.142\n",
      "Por más que hagamos, nos esforcemos o justifiquemos, siempre habrá alguien que nos busque y a quien no le guste lo que hacemos. -> start: 11.142, end: 18.711\n",
      " En el poder de confiar en ti, comparto de la forma más clara y directa todo lo que he vivido y aprendido. -> start: 19.672, end: 27.294\n",
      "No quiero guardarme nada para mí, porque sé que el cambio es posible. -> start: 27.294, end: 32.756\n",
      "También puedes hacerlo tú, independientemente del lugar en el que te encuentres. -> start: 32.756, end: 38.618\n",
      "Recuerda, las puertas del cambio interno están siempre abiertas para todo el que se decida pasar por ellas. -> start: 38.618, end: 45.1\n"
     ]
    }
   ],
   "source": [
    "#pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118 \n",
    "#pip install git+https://github.com/m-bain/whisperx.git\n",
    "\n",
    "import whisperx\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "audio_file = \"audio_input.wav\"\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "# delete model if low on GPU resources\n",
    "#import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "# 2. Align whisper output\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=\"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\", device=device)\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "for i in result[\"segments\"]:\n",
    "    a_traducir = i['text']\n",
    "    print(f\"{i['text']} -> start: {i['start']}, end: {i['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Translate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have always been extremely concerned about being understood and accepted. -> start: 0.59, end: 5.736\n",
      "But I have learned that pleasing or making everyone happy is impossible. -> start: 5.736, end: 11.142\n",
      "No matter how much we do, try hard or justify it, there will always be someone looking for us and who doesn't like what we do. -> start: 11.142, end: 18.711\n",
      "In the power of trusting you, I share in the clearest and most direct way everything I have experienced and learned. -> start: 19.672, end: 27.294\n",
      "I don't want to keep anything to myself, because I know that change is possible. -> start: 27.294, end: 32.756\n",
      "You can do it too, regardless of where you are. -> start: 32.756, end: 38.618\n",
      "Remember, the doors of internal change are always open to anyone who decides to go through them. -> start: 38.618, end: 45.1\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "#pip install googletrans==3.1.0a0\n",
    "\n",
    "for i in result[\"segments\"]:\n",
    "    a_traducir = i['text']\n",
    "\n",
    "    traductor = Translator()\n",
    "\n",
    "    traducido = traductor.translate(a_traducir, dest=\"en\", src=\"es\")\n",
    "    \n",
    "    print(f\"{traducido.text} -> start: {i['start']}, end: {i['end']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text to Speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TSS funciona con la version 3.8\n",
    "pip install TTS\n",
    "\n",
    "Para poder hacer que funcione la libreria transformers funcione tienes que instalar Rust\n",
    "pip install --upgrade transformers scipy\n",
    "\n",
    "para clonar bark en mi archivo:\n",
    "git lfs install\n",
    "git clone https://huggingface.co/suno/bark\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      " > External Speaker Encoder Loaded !!\n",
      " > initialization of language-embedding layers.\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#French, Portgues and English\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False).to(device)\n",
    "#tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API token found for 🐸Coqui Studio voices - https://coqui.ai \n",
      "Visit 🔗https://app.coqui.ai/account to get one.\n",
      "Set it as an environment variable `export COQUI_STUDIO_TOKEN=<token>`\n",
      "\n",
      "['tts_models/multilingual/multi-dataset/xtts_v1', 'tts_models/multilingual/multi-dataset/xtts_v1.1', 'tts_models/multilingual/multi-dataset/your_tts', 'tts_models/multilingual/multi-dataset/bark', 'tts_models/bg/cv/vits', 'tts_models/cs/cv/vits', 'tts_models/da/cv/vits', 'tts_models/et/cv/vits', 'tts_models/ga/cv/vits', 'tts_models/en/ek1/tacotron2', 'tts_models/en/ljspeech/tacotron2-DDC', 'tts_models/en/ljspeech/tacotron2-DDC_ph', 'tts_models/en/ljspeech/glow-tts', 'tts_models/en/ljspeech/speedy-speech', 'tts_models/en/ljspeech/tacotron2-DCA', 'tts_models/en/ljspeech/vits', 'tts_models/en/ljspeech/vits--neon', 'tts_models/en/ljspeech/fast_pitch', 'tts_models/en/ljspeech/overflow', 'tts_models/en/ljspeech/neural_hmm', 'tts_models/en/vctk/vits', 'tts_models/en/vctk/fast_pitch', 'tts_models/en/sam/tacotron-DDC', 'tts_models/en/blizzard2013/capacitron-t2-c50', 'tts_models/en/blizzard2013/capacitron-t2-c150_v2', 'tts_models/en/multi-dataset/tortoise-v2', 'tts_models/en/jenny/jenny', 'tts_models/es/mai/tacotron2-DDC', 'tts_models/es/css10/vits', 'tts_models/fr/mai/tacotron2-DDC', 'tts_models/fr/css10/vits', 'tts_models/uk/mai/glow-tts', 'tts_models/uk/mai/vits', 'tts_models/zh-CN/baker/tacotron2-DDC-GST', 'tts_models/nl/mai/tacotron2-DDC', 'tts_models/nl/css10/vits', 'tts_models/de/thorsten/tacotron2-DCA', 'tts_models/de/thorsten/vits', 'tts_models/de/thorsten/tacotron2-DDC', 'tts_models/de/css10/vits-neon', 'tts_models/ja/kokoro/tacotron2-DDC', 'tts_models/tr/common-voice/glow-tts', 'tts_models/it/mai_female/glow-tts', 'tts_models/it/mai_female/vits', 'tts_models/it/mai_male/glow-tts', 'tts_models/it/mai_male/vits', 'tts_models/ewe/openbible/vits', 'tts_models/hau/openbible/vits', 'tts_models/lin/openbible/vits', 'tts_models/tw_akuapem/openbible/vits', 'tts_models/tw_asante/openbible/vits', 'tts_models/yor/openbible/vits', 'tts_models/hu/css10/vits', 'tts_models/el/cv/vits', 'tts_models/fi/css10/vits', 'tts_models/hr/cv/vits', 'tts_models/lt/cv/vits', 'tts_models/lv/cv/vits', 'tts_models/mt/cv/vits', 'tts_models/pl/mai_female/vits', 'tts_models/pt/cv/vits', 'tts_models/ro/cv/vits', 'tts_models/sk/cv/vits', 'tts_models/sl/cv/vits', 'tts_models/sv/cv/vits', 'tts_models/ca/custom/vits', 'tts_models/fa/custom/glow-tts', 'tts_models/bn/custom/vits-male', 'tts_models/bn/custom/vits-female', 'tts_models/be/common-voice/glow-tts']\n"
     ]
    }
   ],
   "source": [
    "print(TTS().list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I have always been extremely concerned about being understood and accepted.']\n",
      " > Processing time: 1.110597848892212\n",
      " > Real-time factor: 0.24264755274026917\n",
      " > Text splitted to sentences.\n",
      "['But I have learned that pleasing or making everyone happy is impossible.']\n",
      " > Processing time: 1.079387903213501\n",
      " > Real-time factor: 0.21412178203005375\n",
      " > Text splitted to sentences.\n",
      "[\"No matter how much we do, try hard or justify it, there will always be someone looking for us and who doesn't like what we do.\"]\n",
      " > Processing time: 1.1812784671783447\n",
      " > Real-time factor: 0.13325194215209754\n",
      " > Text splitted to sentences.\n",
      "['In the power of trusting you, I share in the clearest and most direct way everything I have experienced and learned.']\n",
      " > Processing time: 1.0580167770385742\n",
      " > Real-time factor: 0.15411752032608508\n",
      " > Text splitted to sentences.\n",
      "[\"I don't want to keep anything to myself, because I know that change is possible.\"]\n",
      " > Processing time: 1.0580382347106934\n",
      " > Real-time factor: 0.20532471079190637\n",
      " > Text splitted to sentences.\n",
      "['You can do it too, regardless of where you are.']\n",
      " > Processing time: 1.1301875114440918\n",
      " > Real-time factor: 0.34113719029402106\n",
      " > Text splitted to sentences.\n",
      "['Remember, the doors of internal change are always open to anyone who decides to go through them.']\n",
      " > Processing time: 1.1590971946716309\n",
      " > Real-time factor: 0.1824487950057659\n"
     ]
    }
   ],
   "source": [
    "contador = 0\n",
    "\n",
    "for i in result[\"segments\"]:\n",
    "    contador += 1\n",
    "\n",
    "    a_traducir = i['text']\n",
    "\n",
    "    traductor = Translator()\n",
    "\n",
    "    traducido = traductor.translate(a_traducir, dest=\"en\", src=\"es\")\n",
    "    \n",
    "    tts.tts_to_file(traducido.text, speaker_wav=f\"bark_voices/juanpedro/sample_audio.wav\", language=\"en\", file_path=f\"output/output{contador}.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
